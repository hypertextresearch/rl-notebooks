{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import wandb\n",
    "from rl.dqn import DQN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# displaying recordings\n",
    "from pyvirtualdisplay import Display\n",
    "import io\n",
    "import base64\n",
    "from gym import wrappers\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f91bdfb59e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = Display(visible=0, size=(600, 600))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITOR_DIR = \"./gym-results\"\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = wrappers.Monitor(env, MONITOR_DIR, video_callable=lambda ep: True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(in_dim, out_dim):\n",
    "    network = nn.Sequential(\n",
    "        nn.Linear(in_dim, 256),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ELU(),\n",
    "        nn.Linear(256, out_dim),\n",
    "    )\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opts = {\n",
    "    \"lr\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_train_hook(obj, variables):\n",
    "    wandb.log({\n",
    "        \"loss\": variables[\"loss\"],\n",
    "        \"step\": variables[\"steps\"],\n",
    "    })\n",
    "    \n",
    "def post_episode_hook(obj, variables, state):\n",
    "    ep = variables[\"ep\"]\n",
    "    ret = variables[\"ep_return\"]\n",
    "    \n",
    "    if ret >= 195 and not state.get(\"saved\", False):\n",
    "        if state.get(\"saved_file\", None) is None:\n",
    "            state[\"saved_file\"] = (\n",
    "                f\"{MONITOR_DIR}/\"\n",
    "                f\"openaigym.video.{env.file_infix}.video{ep:06d}.mp4\"\n",
    "            )\n",
    "        else:\n",
    "            state[\"saved\"] = True\n",
    "            wandb.log({\n",
    "                f\"{wandb.run.name}-ep-{ep}\": wandb.Video(state[\"saved_file\"])\n",
    "            })\n",
    "    \n",
    "    wandb.log({\n",
    "        \"episode\": ep,\n",
    "        \"return\": ret,\n",
    "        \"avg_return\": obj.return_history.mean(),\n",
    "        \"max_return\": obj.return_history.max(),\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env]: num_actions=2\n",
      "[env]: obs_dim=4\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN(env, build_model, optim.Adam, adam_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"adam_opts\": adam_opts\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattfeng\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">generous-rain-32</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mattfeng/deep-q-network\" target=\"_blank\">https://wandb.ai/mattfeng/deep-q-network</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mattfeng/deep-q-network/runs/1u932kcz\" target=\"_blank\">https://wandb.ai/mattfeng/deep-q-network/runs/1u932kcz</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201025_033601-1u932kcz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f91340811d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"deep-q-network\", config=config);\n",
    "wandb.watch(dqn.model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[opt][step 1], still warming up.\n",
      "[opt][step 2], still warming up.\n",
      "[opt][step 3], still warming up.\n",
      "[opt][step 4], still warming up.\n",
      "[opt][step 5], still warming up.\n",
      "[opt][step 6], still warming up.\n",
      "[opt][step 7], still warming up.\n",
      "[opt][step 8], still warming up.\n",
      "[opt][step 9], still warming up.\n",
      "[opt][step 10], still warming up.\n",
      "[opt][step 11], still warming up.\n",
      "[opt][step 12], still warming up.\n",
      "[opt][step 13], still warming up.\n",
      "[opt][step 14], still warming up.\n",
      "[opt][step 15], still warming up.\n",
      "[opt][step 16], still warming up.\n",
      "[opt][step 17], still warming up.\n",
      "[opt][step 18], still warming up.\n",
      "[opt][step 19], still warming up.\n",
      "[opt][step 20], still warming up.\n",
      "[opt][step 21], still warming up.\n",
      "[opt][step 22], still warming up.\n",
      "[opt][step 23], still warming up.\n",
      "[opt][step 24], still warming up.\n",
      "[opt][step 25], still warming up.\n",
      "[opt][step 26], still warming up.\n",
      "[opt][step 27], still warming up.\n",
      "[opt][step 28], still warming up.\n",
      "[opt][step 29], still warming up.\n",
      "[opt][step 30], still warming up.\n",
      "[opt][step 31], still warming up.\n",
      "[opt][step 32], still warming up.\n",
      "[opt][step 33], still warming up.\n",
      "[opt][step 34], still warming up.\n",
      "[opt][step 35], still warming up.\n",
      "[opt][step 36], still warming up.\n",
      "[opt][step 37], still warming up.\n",
      "[opt][step 38], still warming up.\n",
      "[opt][step 39], still warming up.\n",
      "[opt][step 40], still warming up.\n",
      "[opt][step 41], still warming up.\n",
      "[opt][step 42], still warming up.\n",
      "[opt][step 43], still warming up.\n",
      "[opt][step 44], still warming up.\n",
      "[opt][step 45], still warming up.\n",
      "[opt][step 46], still warming up.\n",
      "[opt][step 47], still warming up.\n",
      "[opt][step 48], still warming up.\n",
      "[opt][step 49], still warming up.\n",
      "[opt][step 50], still warming up.\n",
      "[opt][step 51], still warming up.\n",
      "[opt][step 52], still warming up.\n",
      "[opt][step 53], still warming up.\n",
      "[opt][step 54], still warming up.\n",
      "[opt][step 55], still warming up.\n",
      "[opt][step 56], still warming up.\n",
      "[opt][step 57], still warming up.\n",
      "[opt][step 58], still warming up.\n",
      "[opt][step 59], still warming up.\n",
      "[opt][step 60], still warming up.\n",
      "[opt][step 61], still warming up.\n",
      "[opt][step 62], still warming up.\n",
      "[opt][step 63], still warming up.\n",
      "[opt][step 64], still warming up.\n",
      "[opt][step 65], still warming up.\n",
      "[opt][step 66], still warming up.\n",
      "[opt][step 67], still warming up.\n",
      "[opt][step 68], still warming up.\n",
      "[opt][step 69], still warming up.\n",
      "[opt][step 70], still warming up.\n",
      "[opt][step 71], still warming up.\n",
      "[opt][step 72], still warming up.\n",
      "[opt][step 73], still warming up.\n",
      "[opt][step 74], still warming up.\n",
      "[opt][step 75], still warming up.\n",
      "[opt][step 76], still warming up.\n",
      "[opt][step 77], still warming up.\n",
      "[opt][step 78], still warming up.\n",
      "[opt][step 79], still warming up.\n",
      "[opt][step 80], still warming up.\n",
      "[opt][step 81], still warming up.\n",
      "[opt][step 82], still warming up.\n",
      "[opt][step 83], still warming up.\n",
      "[opt][step 84], still warming up.\n",
      "[opt][step 85], still warming up.\n",
      "[opt][step 86], still warming up.\n",
      "[opt][step 87], still warming up.\n",
      "[opt][step 88], still warming up.\n",
      "[opt][step 89], still warming up.\n",
      "[opt][step 90], still warming up.\n",
      "[opt][step 91], still warming up.\n",
      "[opt][step 92], still warming up.\n",
      "[opt][step 93], still warming up.\n",
      "[opt][step 94], still warming up.\n",
      "[opt][step 95], still warming up.\n",
      "[opt][step 96], still warming up.\n",
      "[opt][step 97], still warming up.\n",
      "[opt][step 98], still warming up.\n",
      "[opt][step 99], still warming up.\n",
      "[opt][step 100], still warming up.\n",
      "[opt][step 101], still warming up.\n",
      "[opt][step 102], still warming up.\n",
      "[opt][step 103], still warming up.\n",
      "[opt][step 104], still warming up.\n",
      "[opt][step 105], still warming up.\n",
      "[opt][step 106], still warming up.\n",
      "[opt][step 107], still warming up.\n",
      "[opt][step 108], still warming up.\n",
      "[opt][step 109], still warming up.\n",
      "[opt][step 110], still warming up.\n",
      "[opt][step 111], still warming up.\n",
      "[opt][step 112], still warming up.\n",
      "[opt][step 113], still warming up.\n",
      "[opt][step 114], still warming up.\n",
      "[opt][step 115], still warming up.\n",
      "[opt][step 116], still warming up.\n",
      "[opt][step 117], still warming up.\n",
      "[opt][step 118], still warming up.\n",
      "[opt][step 119], still warming up.\n",
      "[opt][step 120], still warming up.\n",
      "[opt][step 121], still warming up.\n",
      "[opt][step 122], still warming up.\n",
      "[opt][step 123], still warming up.\n",
      "[opt][step 124], still warming up.\n",
      "[opt][step 125], still warming up.\n",
      "[opt][step 126], still warming up.\n",
      "[opt][step 127], still warming up.\n",
      "[opt][step 128], still warming up.\n",
      "[opt][step 129], still warming up.\n",
      "[opt][step 130], still warming up.\n",
      "[opt][step 131], still warming up.\n",
      "[opt][step 132], still warming up.\n",
      "[opt][step 133], still warming up.\n",
      "[opt][step 134], still warming up.\n",
      "[opt][step 135], still warming up.\n",
      "[opt][step 136], still warming up.\n",
      "[opt][step 137], still warming up.\n",
      "[opt][step 138], still warming up.\n",
      "[opt][step 139], still warming up.\n",
      "[opt][step 140], still warming up.\n",
      "[opt][step 141], still warming up.\n",
      "[opt][step 142], still warming up.\n",
      "[opt][step 143], still warming up.\n",
      "[opt][step 144], still warming up.\n",
      "[opt][step 145], still warming up.\n",
      "[opt][step 146], still warming up.\n",
      "[opt][step 147], still warming up.\n",
      "[opt][step 148], still warming up.\n",
      "[opt][step 149], still warming up.\n",
      "[opt][step 150], still warming up.\n",
      "[opt][step 151], still warming up.\n",
      "[opt][step 152], still warming up.\n",
      "[opt][step 153], still warming up.\n",
      "[opt][step 154], still warming up.\n",
      "[opt][step 155], still warming up.\n",
      "[opt][step 156], still warming up.\n",
      "[opt][step 157], still warming up.\n",
      "[opt][step 158], still warming up.\n",
      "[opt][step 159], still warming up.\n",
      "[opt][step 160], still warming up.\n",
      "[opt][step 161], still warming up.\n",
      "[opt][step 162], still warming up.\n",
      "[opt][step 163], still warming up.\n",
      "[opt][step 164], still warming up.\n",
      "[opt][step 165], still warming up.\n",
      "[opt][step 166], still warming up.\n",
      "[opt][step 167], still warming up.\n",
      "[opt][step 168], still warming up.\n",
      "[opt][step 169], still warming up.\n",
      "[opt][step 170], still warming up.\n",
      "[opt][step 171], still warming up.\n",
      "[opt][step 172], still warming up.\n",
      "[opt][step 173], still warming up.\n",
      "[opt][step 174], still warming up.\n",
      "[opt][step 175], still warming up.\n",
      "[opt][step 176], still warming up.\n",
      "[opt][step 177], still warming up.\n",
      "[opt][step 178], still warming up.\n",
      "[opt][step 179], still warming up.\n",
      "[opt][step 180], still warming up.\n",
      "[opt][step 181], still warming up.\n",
      "[opt][step 182], still warming up.\n",
      "[opt][step 183], still warming up.\n",
      "[opt][step 184], still warming up.\n",
      "[opt][step 185], still warming up.\n",
      "[opt][step 186], still warming up.\n",
      "[opt][step 187], still warming up.\n",
      "[opt][step 188], still warming up.\n",
      "[opt][step 189], still warming up.\n",
      "[opt][step 190], still warming up.\n",
      "[opt][step 191], still warming up.\n",
      "[opt][step 192], still warming up.\n",
      "[opt][step 193], still warming up.\n",
      "[opt][step 194], still warming up.\n",
      "[opt][step 195], still warming up.\n",
      "[opt][step 196], still warming up.\n",
      "[opt][step 197], still warming up.\n",
      "[opt][step 198], still warming up.\n",
      "[opt][step 199], still warming up.\n",
      "[ep 9] return=46.0, avg_return=25.22222222222222\n",
      "[ep 19] return=22.0, avg_return=21.63157894736842\n",
      "[ep 29] return=12.0, avg_return=20.96551724137931\n",
      "[ep 39] return=10.0, avg_return=20.46153846153846\n",
      "[ep 49] return=24.0, avg_return=22.346938775510203\n",
      "[ep 59] return=20.0, avg_return=22.338983050847457\n",
      "[ep 69] return=44.0, avg_return=24.07246376811594\n",
      "[ep 79] return=22.0, avg_return=24.949367088607595\n",
      "[ep 89] return=16.0, avg_return=26.876404494382022\n",
      "[ep 99] return=50.0, avg_return=26.242424242424242\n",
      "[ep 109] return=44.0, avg_return=27.07\n",
      "[ep 119] return=29.0, avg_return=30.51\n",
      "[ep 129] return=44.0, avg_return=31.51\n",
      "[ep 139] return=14.0, avg_return=34.14\n",
      "[ep 149] return=138.0, avg_return=37.77\n",
      "[ep 159] return=54.0, avg_return=41.7\n",
      "[ep 169] return=91.0, avg_return=48.15\n",
      "[ep 179] return=200.0, avg_return=60.71\n",
      "[ep 189] return=200.0, avg_return=71.92\n",
      "[ep 199] return=73.0, avg_return=80.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f3aaf60db720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m dqn.train(\n\u001b[1;32m      4\u001b[0m     \u001b[0mpost_train_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_train_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpost_episode_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpost_episode_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/storage/rl/rl/rl/dqn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps_train_interval, steps_warmup, batch_size, discount_factor, epsilon_fn, replay_buffer_size, episodes_target_update, num_episodes, track_length, pre_train_hook, post_train_hook, post_episode_hook)\u001b[0m\n\u001b[1;32m    157\u001b[0m                         \u001b[0mq_est_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_acts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_est_indexed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2133\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_smooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_smooth_l1_loss\u001b[0;34m(input, target)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_smooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[0;31m# type: (Tensor, Tensor) -> Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "\n",
    "dqn.train(\n",
    "    post_train_hook=post_train_hook,\n",
    "    post_episode_hook=lambda a, b: post_episode_hook(a, b, state)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
